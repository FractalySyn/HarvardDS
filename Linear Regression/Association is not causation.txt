rm(list = ls())
x = c("tidyverse", "ggplot2", "imager", "Lahman", "HistData", "broom", "dslabs")
lapply(x, library, character.only = TRUE)
options(digits = 3)


"Association is not causation is perhaps the most important lesson one learns in a statistics
class. Correlation is not causation is another way to say this
There are many reasons that a variable X can be correlated with a variable Y without having
any direct effect on Y . Here we examine three common ways that can lead to misinterpreting
data."




# Spurious Correlation ----------------------------------------------------

"Spurious = false"
# The following comical example underscores that correlation is not causation. It shows a very
# strong correlation between divorce rates and margarine consumption.

"https://rafalab.github.io/dsbook/book_files/figure-html/divorce-versus-margarine-1.png" %>%
   load.image() %>% plot(axes = F)

# Does this mean that margarine causes divorces? Or do divorces cause people to eat more
# margarine? Of course the answer to both these questions is no. This is just an example of
# what we call a spurious correlation.

"http://tylervigen.com/spurious-correlations"
# The cases presented in the spurious correlation site are all instances of what is generally
# called data dredging, data fishing, or data snooping. It’s basically a form of what in the
# US they call cherry picking. An example of data dredging would be if you look through
# many results produced by a random process and pick the one that shows a relationship that
# supports a theory you want to defend.

"A Monte Carlo simulation can be used to show how data dredging can result in finding high
correlations among uncorrelated variables. We will save the results of our simulation into a
tibble:"
# Le data dredging (en anglais « dragage de données ») est une technique statistique 
# qui « consiste à ne publier que les compositions d'échantillon et les périodes 
# d'observation favorables à l'hypothèse testée ».
N <- 25
g <- 100000
sim_data <- tibble(group = rep(1:g, each=N),
                   x = rnorm(N * g),
                   y = rnorm(N * g))
"The first column denotes group. We created groups and for each one we generated a pair of
independent vectors, X and Y , with 25 observations each, stored in the second and third
columns. Because we constructed the simulation, we know that X and Y are not correlated"
res = sim_data %>%
   group_by(group) %>%
   summarize(r = cor(x, y)) %>%
   arrange(desc(r))
# We see a maximum correlation of 0.789 and if you just plot the data from the group achieving
# this correlation, it shows a convincing plot that X and Y are in fact correlated:
sim_data %>% filter(group == res$group[which.max(res$r)]) %>%
   ggplot(aes(x, y)) +
   geom_point() +
   geom_smooth(method = "lm")

"Remember that the correlation summary is a random variable. Here is the distribution
generated by the Monte Carlo simulation:"
res %>% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = "black")
# It’s just a mathematical fact that if we observe random correlations that are expected to be
# 0, but have a standard error of 0.204, the largest one will be close to 1.

"If we performed regression on this group and interpreted the p-value, we would incorrectly
claim this was a statistically significant relation:"
sim_data %>%
   filter(group == res$group[which.max(res$r)]) %>%
   do(tidy(lm(y ~ x, data = .))) %>%
   filter(term == "x")

"!!!!!!!!!!!!!!!
This particular form of data dredging is referred to as p-hacking. P-hacking is a topic of
much discussion because it is a problem in scientific publications. Because publishers tend
to reward statistically significant results over negative results, there is an incentive to report
significant results. Furthermore, they might try fitting several
different models to account for confounding and pick the one that yields the smallest p-value.
In experimental disciplines, an experiment might be repeated more than once, yet only
the results of the one experiment with a small p-value reported. This does not necessarily
happen due to unethical behavior, but rather as a result of statistical ignorance or wishful
thinking. In advanced statistics courses, you can learn methods to adjust for these multiple
comparisons.
!!!!!!!!!!!!!!!!"






# Outliers ----------------------------------------------------------------

# Suppose we take measurements from two independent outcomes, X and Y , and we standardize
# the measurements. However, imagine we make a mistake and forget to standardize
# entry 23. We can simulate such data using:
set.seed(1985)
x <- rnorm(100,100,1)
y <- rnorm(100,84,1)
x[-23] <- scale(x[-23])
y[-23] <- scale(y[-23])

"The data look like this:"
qplot(x, y)
"Not surprisingly, the correlation is very high:"
cor(x, y)

# But this is driven by the one outlier. If we remove this outlier, the correlation is greatly
# reduced to almost 0, which is what it should be:
cor(x[-23], y[-23])


"In Section 11 we described alternatives to the average and standard deviation that are
robust to outliers. There is also an alternative to the sample correlation for estimating the
population correlation that is robust to outliers. It is called Spearman correlation. The idea
is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks
plotted against each other:"
qplot(rank(x), rank(y))
# The outlier is no longer associated with a very large value and the correlation comes way
# down:
cor(rank(x), rank(y))
"Spearman correlation can also be calculated like this:"
cor(x, y, method = "spearman")


# There are also methods for robust fitting of linear models which you can learn about in, for
# instance, this book: Robust Statistics: Edition 2 by Peter J. Huber & Elvezio M. Ronchetti.




# Reversing cause and effect ----------------------------------------------

"Another way association is confused with causation is when the cause and effect are reversed.
An example of this is claiming that tutoring makes students perform worse because they
test lower than peers that are not tutored. In this case, the tutoring is not causing the low
test scores, but the other way around."

# We can easily construct an example of cause and effect reversal using the father and son
# height data.

library(HistData)
data("GaltonFamilies")
GaltonFamilies %>%
   filter(childNum == 1 & gender == "male") %>%
   select(father, childHeight) %>%
   rename(son = childHeight) %>%
   do(tidy(lm(father ~ son, data = .)))
"The model fits the data very well. If we look at the mathematical formulation of the model
above, it could easily be incorrectly interpreted so as to suggest that the son being tall
caused the father to be tall. But given what we know about genetics and biology, we know
it’s the other way around. The model is technically correct. The estimates and p-values were
obtained correctly as well. What is wrong here is the interpretation."





# Confounders = control vairaibles -------------------------------------------------------------

"Confounders are perhaps the most common reason that leads to associations begin misinterpreted.

If X and Y are correlated, we call Z a confounder if changes in Z causes changes in both
X and Y . Earlier, when studying baseball data, we saw how Home Runs was a confounder
that resulted in a higher correlation than expected when studying the relationship between
Bases on Balls and Runs. In some cases, we can use linear models to account for confounders.
However, this is not always the case.

Incorrect interpretation due to confounders is ubiquitous in the lay press and they are often
hard to detect. Here, we present a widely used example related to college admissions."


# Admission data from six U.C. Berkeley majors, from 1973, showed that more men were
# being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel,
# EA Hammel, and JW O’Connell. Science (1975). We can load the data and a statistical test,
# which clearly rejects the hypothesis that gender and admission are independent:
data(admissions)
admissions %>% group_by(gender) %>%
   summarize(total_admitted = round(sum(admitted / 100 * applicants)),
             not_admitted = sum(applicants) - sum(total_admitted)) %>%
   select(-gender) %>%
   do(tidy(chisq.test(.))) %>% .$p.value

"But closer inspection shows a paradoxical result. Here are the percent admissions by major:"
admissions %>% select(major, gender, admitted) %>%
   spread(gender, admitted) %>%
   mutate(women_minus_men = women - men)
# Four out of the six majors favor women. More importantly, all the differences are much
# smaller than the 14.2 difference that we see when examining the totals.
# The paradox is that analyzing the totals suggests a dependence between admission and
# gender, but when the data is grouped by major, this dependence seems to disappear. What’s
# going on? This actually can happen if an uncounted confounder is driving most of the
# variability.





"So let’s define three variables: X is 1 for men and 0 for women, Y is 1 for those admitted
and 0 otherwise, and Z quantifies the selectivity of the major. A gender bias claim would
be based on the fact that Pr(Y = 1|X = x) is higher for x = 1 than x = 0. However, Z is
an important confounder to consider. Clearly Z is associated with Y , as the more selective
a major, the lower Pr(Y = 1|Z = z). But is major selectivity Z associated with gender X?"
# One way to see this is to plot the total percent admitted to a major versus the percent of
# women that made up the applicants:
admissions %>%
   group_by(major) %>%
   summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),
             percent_women_applicants = sum(applicants * (gender=="women")) /
                sum(applicants) * 100) %>%
   ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +
   geom_text()

"There seems to be association. The plot suggests that women were much more likely to
apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare,
for example, major B and major E. Major E is much harder to enter than major B and over
60% of applicants to major E were women, while less than 30% of the applicants of major
B were women."


# The following plot shows the number of applicants that were admitted and those that were
# not:
"https://rafalab.github.io/dsbook/book_files/figure-html/confounding-1.png" %>%
   load.image() %>% plot(axes = F)
# It also breaks down the acceptances by major. This breakdown allows us to see that the
# majority of accepted men came from two majors: A and B. It also lets us see that few women
# applied to these majors.


"In this plot, we can see that if we condition or stratify by major, and then look at differences,
we control for the confounder and this effect goes away:"
admissions %>%
   ggplot(aes(major, admitted, col = gender, size = applicants)) +
   geom_point()
# Now we see that major by major, there is not much difference. The size of the dot represents
# the number of applicants, and explains the paradox: we see large red dots and small blue
# dots for the easiest majors, A and B.

"If we average the difference by major, we find that the percent is actually 3.5% higher for
women."
admissions %>% group_by(gender) %>% summarize(average = mean(admitted))






# Simpson's paradox -------------------------------------------------------

"The case we have just covered is an example of Simpson’s paradox. It is called a paradox
because we see the sign of the correlation flip when comparing the entire publication and
specific strata. As an illustrative example, suppose you have three random variables X, Y ,
and Z and that we observe realizations of these. Here is a plot of simulated observations for
X and Y along with the sample correlation:"

"https://rafalab.github.io/dsbook/book_files/figure-html/simpsons-paradox-1.png" %>%
   load.image() %>% plot(axes = F)

# You can see that X and Y are negatively correlated. However, once we stratify by Z (shown
# in different colors below) another pattern emerges:
"https://rafalab.github.io/dsbook/book_files/figure-html/simpsons-paradox-explained-1.png" %>%
   load.image() %>% plot(axes = F)

"!!!!!
It is really Z that is negatively correlated with X. If we stratify by Z, the X and Y are
actually positively correlated as seen in the plot above.
!!!!!"







# Assessment: Correlation is Not Causation --------------------------------

"Q1 In the videos, we ran one million tests of correlation for two random variables,
X and Y.
How many of these correlations would you expect to have a significant p-value 
( p≤0.05 ), just by chance?"
0.05*1000000

"Q2 Which of the following are examples of p-hacking?"
Looking for associations between an outcome and several exposures and only reporting the one that is significant.
Trying several different models and selecting the one that yields the smallest p-value.
Repeating an experiment multiple times and only reporting the one with the smallest p-value.

"Q3 The Spearman correlation coefficient is robust to outliers because:"
It calculates correlation between ranks, not values.

"Q4 What can you do to determine if you are misinterpreting results because of a confounder?"
More closely examine the results by stratifying and plotting the data.

"Q5 Look again at the admissions data presented in the confounders video using ?admissions.
What important characteristic of the table variables do you need to know to understand the
calculations used in this video?"
?admissions
The column admitted is the percent of students admitted, while the column applicants is the
total number of applicants.

"Q6 In the example in the confounders video, major selectivity confounds the relationship
between UC Berkeley admission rates and gender because:"
Major selectivity is associated with both admission rates and with gender, as women 
tended to apply to more selective majors.

"Q7 Admission rates at UC Berkeley are an example of Simpson’s Paradox because:"
It appears that men have higher a higher admission rate than women, however, after 
we stratify by major, we see that on average women have a higher admission rate than men.




# Assessment: Confounding -------------------------------------------------


"For this set of exercises, we examine the data from a 2014 PNAS paper that analyzed 
success rates from funding agencies in the Netherlands External link and concluded:"
# "our results reveal gender bias favoring male applicants over female applicants in the 
# prioritization of their "quality of researcher" (but not "quality of proposal") evaluations
# and success rates, as well as in the language used in instructional and evaluation materials."

"A response was published a few months later titled No evidence that gender contributes to personal
research funding success in The Netherlands: A reaction to Van der Lee and Ellemers External link,
which concluded:"
# However, the overall gender effect borders on statistical significance, despite the 
# large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox;
# if a higher percentage of women apply for grants in more competitive scientific 
# disciplines (i.e., with low application success rates for both men and women), then
# an analysis across all disciplines could incorrectly show "evidence" of gender inequality. 


"Who is right here: the original paper or the response? Here, you will examine the data 
and come to your own conclusion."
# The main evidence for the conclusion of the original paper comes down to a comparison of
# the percentages. The information we need was originally in Table S1 in the paper, which
# we include in dslabs:
library(dslabs)
data("research_funding_rates")
research_funding_rates

"Construct a two-by-two table of gender (men/women) by award status (awarded/not)
using the total numbers across all disciplines."
research_funding_rates %>%
   summarise(men = sum(applications_men),
             women = sum(applications_women),
             yes_men = sum(awards_men),
             no_men = men - yes_men,
             yes_women = sum(awards_women),
             no_women = women - yes_women)
twobytwo = data.frame(awarded = c(290, 177),
                      not_awarded = c(1345, 1011),
                      row.names = c("men", "women"))
twobytwo

"Q2 Use the two-by-two table from Question 1 to compute the percentages of men
awarded versus women awarded."
twobytwo %>%
   mutate(percentage = 100*awarded/(awarded+not_awarded))


"Q3 Run a chi-squared test External link on the two-by-two table to determine 
whether the difference in the two success rates is significant. (You can use 
tidy() to turn the output of chisq.test() into a data frame as well.)"
twobytwo %>% 
   chisq.test() %>%
   tidy()

"There may be an association between gender and funding. But can we infer causation here?
Is gender bias causing this observed difference? The response to the original paper claims
that what we see here is similar to the UC Berkeley admissions example. Specifically they
state that this could be a prime example of Simpson’s paradox; if a higher percentage of
women apply for grants in more competitive scientific disciplines, then an analysis across
all disciplines could incorrectly show 'evidence' of gender inequality."
# To settle this dispute, use this dataset with number of applications, awards, and success
# rate for each gender:
dat <- research_funding_rates %>% 
   mutate(discipline = reorder(discipline, success_rates_total)) %>%
   rename(success_total = success_rates_total,
          success_men = success_rates_men,
          success_women = success_rates_women) %>%
   gather(key, value, -discipline) %>%
   separate(key, c("type", "gender")) %>%
   spread(type, value) %>%
   filter(gender != "total")
dat

"To check if this is a case of Simpson's paradox, plot the success rates versus 
disciplines, which have been ordered by overall success, with colors to denote 
the genders and size to denote the number of applications."
dat %>%
   ggplot(aes(discipline, success, size = applications, color = gender)) +
   geom_point()


















